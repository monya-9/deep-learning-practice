{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOLtztT+Js2sur1qbujUxC1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monya-9/deep-learning-practice/blob/main/12_huggingface_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HuggingFace Transformers로 BERT"
      ],
      "metadata": {
        "id": "S3maTpkI7Lee"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQls7N6F7BAp",
        "outputId": "f733e9c0-c661-4da4-d12d-3d6e0d8c0001"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "# 1. 라이브러리 설치\n",
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 토크나이저와 모델 불러오기\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# BERT-base-uncased 모델 사용\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzO8kdcB7h8l",
        "outputId": "6831a4cf-2830-482d-ab5d-6bded1b15aa0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- BERT-base (uncased) 모델과 토크나이저 로드"
      ],
      "metadata": {
        "id": "Ygmd96j68X3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 문장 준비\n",
        "sentence = \"Hugging Face BERT practice is interesting!\""
      ],
      "metadata": {
        "id": "WKQq1MT67jLY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 토큰화 및 인코딩\n",
        "inputs = tokenizer(sentence, return_tensors=\"pt\")  # PyTorch tensor 반환\n",
        "print(\"입력 토큰 ID:\", inputs[\"input_ids\"])\n",
        "print(\"어텐션 마스크:\", inputs[\"attention_mask\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPM6xTar7loM",
        "outputId": "33d86fc5-6fe9-4724-8518-68ec3de6b1cf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 토큰 ID: tensor([[  101, 17662,  2227, 14324,  3218,  2003,  5875,   999,   102]])\n",
            "어텐션 마스크: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 단어를 토큰 단위로 분리하고 정수 ID 변환\n",
        "- [CLS], [SEP] 포함"
      ],
      "metadata": {
        "id": "XGrdlw6D8b_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 모델 실행 (forward pass)\n",
        "with torch.no_grad():  # 학습 X, 추론만\n",
        "    outputs = model(**inputs)"
      ],
      "metadata": {
        "id": "gfoIIMgT7mfa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 입력을 BERT에 넣어 단어 단위(hidden state)와 문장 임베딩(pooler_output)을 출력"
      ],
      "metadata": {
        "id": "tdAIcBpD8iok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. 출력 확인\n",
        "print(\"\\n=== 출력 ===\")\n",
        "print(\"last_hidden_state:\", outputs.last_hidden_state.shape)\n",
        "print(\"pooler_output:\", outputs.pooler_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xu3vif6Q7njd",
        "outputId": "57b5e52c-534f-4fd6-c881-4733b85476eb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== 출력 ===\n",
            "last_hidden_state: torch.Size([1, 9, 768])\n",
            "pooler_output: torch.Size([1, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- outputs.last_hidden_state: 모든 토큰(hidden state) 벡터 (배치 크기 x 토큰 개수 x 768차원)\n",
        "- outputs.pooler_output: [CLS] 토큰 기반 문장 벡터 ( 배치 크기 x 768차원)"
      ],
      "metadata": {
        "id": "hdA_ieV18nua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. 문장 임베딩 (pooler_output 사용)\n",
        "sentence_embedding = outputs.pooler_output\n",
        "print(\"\\n문장 임베딩 벡터 차원:\", sentence_embedding.shape)\n",
        "print(sentence_embedding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuT2s4Gf7odA",
        "outputId": "7f7d5d2c-234d-4e3d-f9c2-4da7015968b0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "문장 임베딩 벡터 차원: torch.Size([1, 768])\n",
            "tensor([[-7.6616e-01, -1.7362e-01,  5.9557e-01,  3.8121e-01, -3.1588e-01,\n",
            "         -1.5283e-02,  8.0492e-01,  1.5867e-01,  4.2045e-01, -9.9902e-01,\n",
            "          3.5151e-01, -1.3399e-01,  9.6092e-01, -3.8096e-01,  8.9829e-01,\n",
            "         -3.4242e-01,  1.6761e-01, -4.5077e-01,  2.4046e-01, -4.9891e-01,\n",
            "          4.7402e-01,  6.4965e-01,  6.6592e-01,  1.8362e-01,  2.5549e-01,\n",
            "         -1.1114e-01, -4.4155e-01,  8.8547e-01,  9.0434e-01,  5.5801e-01,\n",
            "         -5.0125e-01,  1.1161e-01, -9.6717e-01, -1.0528e-01,  5.6874e-01,\n",
            "         -9.5737e-01,  4.2823e-02, -6.5959e-01,  6.3764e-02,  1.1802e-01,\n",
            "         -8.3111e-01,  1.2409e-01,  9.8176e-01, -4.9814e-01, -2.4154e-01,\n",
            "         -2.6183e-01, -9.9652e-01,  1.3074e-01, -7.7726e-01, -5.4344e-01,\n",
            "         -4.2838e-01, -6.4672e-01, -2.5718e-02,  2.3404e-01,  2.3145e-01,\n",
            "          4.7355e-01, -2.0612e-01,  3.4768e-02, -4.7149e-02, -4.1010e-01,\n",
            "         -5.3218e-01,  1.2007e-01,  2.1638e-01, -8.2388e-01, -4.7633e-01,\n",
            "         -6.3906e-01, -3.3259e-02, -1.5136e-01,  1.5056e-02, -5.8037e-02,\n",
            "          7.4859e-01,  8.1229e-02,  3.1495e-01, -7.6261e-01, -5.8932e-01,\n",
            "          8.4296e-02, -2.6088e-01,  9.9990e-01, -8.1692e-02, -9.4542e-01,\n",
            "         -4.0791e-01, -3.6827e-01,  1.5582e-01,  6.8713e-01, -5.7045e-01,\n",
            "         -9.9948e-01,  1.0184e-01, -1.7185e-02, -9.7433e-01,  1.3196e-01,\n",
            "          2.0841e-01, -7.1370e-02, -5.3670e-01,  1.8938e-01, -2.2283e-02,\n",
            "          1.1086e-02, -1.0513e-01,  4.7060e-01, -6.4187e-03,  1.2635e-01,\n",
            "         -9.6167e-02, -1.0334e-01,  9.3310e-02, -2.1838e-01,  4.8775e-02,\n",
            "         -2.4259e-01, -3.2622e-01,  6.4812e-02, -3.1756e-01,  5.1913e-01,\n",
            "          1.7788e-01, -1.5574e-01,  1.2428e-01, -9.1602e-01,  4.5659e-01,\n",
            "         -1.5294e-01, -9.5967e-01, -2.3350e-01, -9.7134e-01,  5.1527e-01,\n",
            "          1.6654e-01, -1.0864e-01,  9.2235e-01,  6.3919e-01,  1.2815e-01,\n",
            "          1.0306e-01,  5.7032e-01, -9.9996e-01, -6.1862e-02, -2.1092e-03,\n",
            "          1.7933e-01,  2.7206e-02, -9.4437e-01, -9.0189e-01,  3.8917e-01,\n",
            "          9.1497e-01,  4.8445e-02,  9.8232e-01, -5.7584e-02,  8.7062e-01,\n",
            "          3.2892e-01,  1.5969e-01, -4.3150e-01, -2.3298e-01,  1.6218e-01,\n",
            "          3.1556e-02, -3.8275e-01,  1.2285e-01,  2.9610e-01, -2.5846e-01,\n",
            "          6.7267e-02, -1.0085e-01,  5.5686e-01, -9.0068e-01, -2.8515e-01,\n",
            "          8.9205e-01,  3.8707e-01,  6.3131e-01,  7.2070e-01, -8.0470e-02,\n",
            "         -1.8878e-01,  7.1713e-01,  2.7014e-03,  2.2258e-01,  6.9321e-02,\n",
            "          2.1901e-01, -2.9890e-01,  2.8816e-01, -6.9347e-01,  3.4562e-01,\n",
            "          1.7113e-01, -8.3902e-02,  5.6711e-01, -9.4553e-01, -1.6665e-01,\n",
            "          3.1658e-01,  9.6507e-01,  6.5278e-01,  7.2206e-02, -2.1672e-01,\n",
            "         -8.5697e-02,  6.2941e-02, -8.8454e-01,  9.4061e-01, -5.2940e-02,\n",
            "          1.4662e-01,  4.6340e-01, -5.8770e-02, -7.1380e-01, -4.4770e-01,\n",
            "          7.0654e-01,  9.9685e-02, -7.0392e-01,  1.4530e-01, -3.0177e-01,\n",
            "         -2.8761e-01,  3.1248e-01,  2.0080e-01, -1.9386e-01, -3.1116e-01,\n",
            "          9.4253e-02,  8.8660e-01,  8.8839e-01,  6.3585e-01, -6.4414e-01,\n",
            "          2.7769e-01, -8.2562e-01, -1.9378e-01,  2.5017e-02,  9.7818e-02,\n",
            "         -1.0850e-02,  9.8239e-01,  1.1363e-01, -7.2862e-03, -8.5844e-01,\n",
            "         -9.6360e-01, -9.5452e-02, -7.5730e-01,  1.1773e-01, -4.9436e-01,\n",
            "          6.8555e-02,  5.2084e-01, -2.9491e-01,  2.2338e-01, -9.3841e-01,\n",
            "         -6.3842e-01,  2.0971e-01, -4.3585e-02,  2.9172e-01, -1.8324e-01,\n",
            "          4.2209e-01, -4.2042e-01, -4.0377e-01,  5.8771e-01,  8.1899e-01,\n",
            "          5.4646e-01, -5.6449e-01,  6.6155e-01, -1.0709e-01,  7.4905e-01,\n",
            "         -3.9083e-01,  9.1477e-01, -3.5779e-01,  2.1099e-01, -8.6007e-01,\n",
            "          4.6579e-01, -7.6307e-01,  3.5899e-01, -4.3966e-03, -6.9968e-01,\n",
            "         -4.5459e-01,  2.1711e-01,  1.6419e-01,  8.1278e-01, -2.8576e-01,\n",
            "          9.7952e-01, -6.1446e-01, -8.8878e-01,  1.7348e-01,  1.0708e-01,\n",
            "         -9.6192e-01, -2.7478e-01,  1.6743e-01, -6.1048e-01, -2.1703e-01,\n",
            "         -2.8043e-01, -9.0817e-01,  6.8894e-01, -8.8122e-03,  9.4934e-01,\n",
            "          1.1781e-01, -7.6111e-01, -1.6017e-01, -8.4015e-01, -2.7970e-01,\n",
            "          5.1721e-03,  6.9776e-01, -1.8328e-01, -9.2653e-01,  3.1043e-01,\n",
            "          3.4903e-01,  2.6927e-01,  6.3573e-01,  9.7853e-01,  9.8155e-01,\n",
            "          9.4454e-01,  8.1178e-01,  7.8111e-01, -4.8074e-01, -1.3388e-01,\n",
            "          9.9959e-01,  7.0887e-03, -9.9861e-01, -8.6444e-01, -3.7861e-01,\n",
            "          2.2662e-01, -9.9994e-01, -2.9858e-02,  7.6297e-02, -8.4587e-01,\n",
            "         -5.0248e-01,  9.5370e-01,  9.4679e-01, -9.9985e-01,  7.4709e-01,\n",
            "          8.6665e-01, -2.9177e-01, -1.0064e-01, -6.3621e-02,  9.3644e-01,\n",
            "          2.0474e-01,  3.5223e-01, -1.3863e-01,  1.9860e-01,  2.9710e-01,\n",
            "         -6.6758e-01,  5.9655e-01,  4.8509e-01,  8.7107e-02,  4.1396e-02,\n",
            "         -4.6766e-01, -8.6292e-01, -4.0898e-01, -9.4687e-02, -4.6022e-01,\n",
            "         -9.2117e-01, -5.8971e-02, -4.6292e-01,  4.2493e-01, -4.0422e-02,\n",
            "          6.2229e-02, -5.9967e-01,  5.7633e-02, -7.3963e-01,  2.8263e-01,\n",
            "          3.6981e-01, -9.0548e-01, -4.0239e-01,  2.1094e-01, -5.0543e-01,\n",
            "          4.0185e-01, -9.0628e-01,  9.3395e-01, -1.9903e-01, -4.2704e-01,\n",
            "          9.9988e-01, -3.4300e-01, -7.4238e-01,  1.4337e-01, -1.5652e-02,\n",
            "          2.3362e-03,  9.9971e-01,  2.0405e-01, -9.4927e-01, -2.4809e-01,\n",
            "          5.8731e-02, -2.5708e-01, -1.5375e-01,  9.9108e-01, -6.4725e-02,\n",
            "          5.1221e-01,  5.5604e-01,  9.2119e-01, -9.6789e-01, -1.7988e-01,\n",
            "         -8.1859e-01, -9.2403e-01,  9.2647e-01,  8.7851e-01,  8.2779e-02,\n",
            "         -4.3974e-01,  7.4008e-03,  2.7407e-01,  1.0707e-01, -9.1426e-01,\n",
            "          2.9695e-01,  2.1059e-01, -6.5238e-02,  8.2249e-01, -6.1680e-01,\n",
            "         -2.0745e-01,  2.5926e-01,  2.5323e-01,  4.5087e-01, -4.6317e-01,\n",
            "          3.0685e-01, -7.7308e-02, -1.0252e-02, -1.4785e-01, -1.3213e-01,\n",
            "         -9.4003e-01, -2.5727e-01,  9.9970e-01,  1.3619e-01, -6.2418e-01,\n",
            "          2.4521e-02, -2.0607e-02, -3.1584e-01,  1.9384e-01,  2.4313e-01,\n",
            "         -1.5567e-01, -7.6061e-01, -4.0166e-01, -8.6186e-01, -9.5961e-01,\n",
            "          5.6473e-01,  6.7075e-02, -1.7202e-01,  9.7989e-01,  4.6401e-03,\n",
            "          3.8359e-02, -1.8851e-01, -2.3186e-01, -6.3286e-02,  4.0944e-01,\n",
            "         -6.2550e-01,  9.2943e-01, -1.2163e-01,  1.9103e-01,  6.7114e-01,\n",
            "          5.8219e-01, -1.7462e-01, -4.6672e-01, -9.4364e-03, -8.4491e-01,\n",
            "          1.6366e-01, -8.9451e-01,  9.1572e-01, -6.1927e-01,  1.3457e-01,\n",
            "          6.3646e-02, -4.2440e-01,  9.9982e-01,  1.1578e-01,  4.1013e-01,\n",
            "         -1.9040e-01,  6.8876e-01, -3.9626e-01, -5.6600e-01, -2.4716e-01,\n",
            "          1.1669e-01,  6.3801e-01, -1.4299e-01,  9.0144e-02, -9.2311e-01,\n",
            "         -5.5628e-01, -4.0739e-01, -9.3622e-01, -9.7403e-01,  6.1031e-01,\n",
            "          4.6305e-01, -3.3117e-02,  1.0833e-01, -4.7869e-01, -4.6546e-01,\n",
            "         -7.9727e-02, -1.5645e-02, -8.7937e-01,  6.6787e-01, -6.6963e-02,\n",
            "          2.9306e-01, -1.5189e-01,  2.5834e-01, -5.9877e-01,  8.2078e-01,\n",
            "          6.4921e-01,  1.9318e-01,  7.3682e-03, -6.5211e-01,  5.5709e-01,\n",
            "         -6.2567e-01,  4.1389e-01,  2.3767e-02,  9.9993e-01, -1.9208e-01,\n",
            "         -3.7835e-01,  5.8044e-01,  4.6653e-01,  2.4917e-02,  9.1853e-02,\n",
            "         -3.9781e-01,  8.6446e-02,  5.2587e-01,  6.5908e-01, -4.9723e-01,\n",
            "         -1.6837e-01,  3.3196e-01, -5.5436e-01, -5.2178e-01,  6.1860e-01,\n",
            "          2.1490e-02, -5.6769e-03,  4.6094e-02, -4.8006e-02,  9.8975e-01,\n",
            "          2.2294e-03, -3.6199e-02, -2.9333e-01,  1.0130e-01, -2.0512e-01,\n",
            "         -2.2576e-01,  9.9926e-01,  1.4341e-01, -2.9153e-01, -9.6867e-01,\n",
            "          4.0908e-01, -7.5673e-01,  9.6175e-01,  6.9300e-01, -6.7528e-01,\n",
            "          2.3076e-01,  7.5981e-02, -2.5447e-03,  5.2733e-01, -2.6369e-02,\n",
            "         -1.9397e-01,  9.0361e-02,  9.1652e-02,  9.1328e-01, -3.2895e-01,\n",
            "         -9.1701e-01, -3.8815e-01,  1.9811e-01, -9.1926e-01,  5.5311e-01,\n",
            "         -3.1843e-01, -6.9450e-02, -1.8960e-01,  3.9032e-01,  5.1242e-01,\n",
            "         -1.2360e-01, -9.5481e-01, -2.6449e-02, -9.6381e-02,  9.1135e-01,\n",
            "          1.7820e-02, -1.9570e-01, -8.1425e-01, -6.0633e-01, -3.2400e-01,\n",
            "          5.3365e-01, -8.8924e-01,  9.3195e-01, -9.5630e-01,  1.9860e-01,\n",
            "          9.9868e-01,  1.7880e-01, -7.1972e-01,  6.9143e-03, -2.6008e-01,\n",
            "          1.4782e-01,  3.6420e-01,  3.1009e-01, -9.1390e-01, -1.8523e-01,\n",
            "         -9.8169e-02,  1.1616e-01, -1.8747e-02,  4.0748e-01,  5.3236e-01,\n",
            "          1.1650e-01, -1.4033e-01, -3.8217e-01,  7.6607e-02,  2.7814e-01,\n",
            "          5.0703e-01, -2.1794e-01,  5.7396e-03, -1.8127e-02, -3.1425e-02,\n",
            "         -8.3446e-01, -5.8803e-02, -4.2267e-03, -8.1039e-01,  5.0243e-01,\n",
            "         -9.9987e-01, -4.9487e-01, -6.3841e-01, -5.9151e-02,  6.8445e-01,\n",
            "          2.5483e-01, -3.5598e-01, -5.3568e-01,  5.5778e-01,  8.4181e-01,\n",
            "          5.8206e-01,  3.3340e-02,  3.6161e-01, -5.1887e-01,  1.7364e-02,\n",
            "          1.5775e-02,  1.6189e-01,  3.4530e-01,  6.1806e-01, -9.2301e-02,\n",
            "          9.9994e-01, -6.3280e-03, -2.0331e-01, -9.0487e-01,  1.4377e-01,\n",
            "         -7.2413e-02,  9.9148e-01, -7.7319e-01, -8.8576e-01,  2.0175e-01,\n",
            "         -1.6781e-01, -6.8749e-01,  1.0839e-01, -4.1617e-02, -4.6110e-01,\n",
            "          3.4807e-01,  9.1543e-01,  7.0037e-01, -2.8033e-01,  1.9902e-01,\n",
            "         -1.4175e-01, -2.2725e-01, -1.0666e-01, -5.8623e-01,  9.6938e-01,\n",
            "          1.5385e-01,  7.3896e-01,  4.8607e-01,  2.2519e-01,  9.2359e-01,\n",
            "          4.0196e-02,  2.7033e-01, -5.1561e-02,  9.9911e-01,  1.2111e-01,\n",
            "         -8.4273e-01,  4.8343e-01, -9.6087e-01, -3.4154e-02, -8.9363e-01,\n",
            "          1.2195e-01, -2.0810e-03,  7.9024e-01, -1.3518e-01,  8.9219e-01,\n",
            "          6.5218e-01, -4.0440e-02,  3.3118e-01,  7.2196e-01,  2.5578e-01,\n",
            "         -8.4478e-01, -9.6451e-01, -9.6920e-01,  8.7362e-02, -3.3397e-01,\n",
            "          5.3059e-02,  1.6197e-01,  5.1040e-02,  1.9605e-01,  2.9618e-01,\n",
            "         -9.9860e-01,  8.4941e-01,  2.2413e-01, -4.6790e-01,  9.2187e-01,\n",
            "          6.5685e-02,  1.4040e-02,  9.8131e-02, -9.6398e-01, -9.0302e-01,\n",
            "         -2.6019e-01, -1.5436e-01,  5.4455e-01,  4.3447e-01,  7.5024e-01,\n",
            "          2.0348e-01, -3.6834e-01, -1.3963e-01,  5.6537e-01, -5.1883e-01,\n",
            "         -9.7984e-01,  2.2291e-01,  5.0742e-01, -8.7505e-01,  9.1461e-01,\n",
            "         -5.5791e-01, -9.0532e-02,  6.5140e-01,  2.4091e-01,  8.4236e-01,\n",
            "          6.3742e-01,  1.9431e-01,  8.5074e-02,  2.8868e-01,  7.9336e-01,\n",
            "          8.7532e-01,  9.6160e-01,  4.2533e-01,  5.7132e-01,  5.4982e-01,\n",
            "          2.7978e-01,  3.0936e-01, -8.8202e-01, -1.8883e-02, -1.6768e-01,\n",
            "          1.8946e-01,  6.9198e-02, -3.7679e-02, -9.0102e-01,  1.5854e-01,\n",
            "         -4.8061e-03,  2.8518e-01, -2.6299e-01,  2.0903e-01, -2.6581e-01,\n",
            "         -9.5550e-03, -5.0925e-01, -2.4474e-01,  3.4314e-01, -3.4728e-02,\n",
            "          8.2817e-01,  1.9033e-01,  3.5628e-02, -1.9498e-01, -1.4955e-02,\n",
            "          4.8253e-01, -8.5045e-01,  7.9945e-01,  9.3650e-02,  5.1569e-01,\n",
            "         -4.7769e-01, -1.8457e-01,  5.5680e-01, -3.9714e-01, -2.3186e-01,\n",
            "         -1.5720e-01, -5.3438e-01,  7.6451e-01,  1.0020e-01, -2.9372e-01,\n",
            "         -2.0822e-01,  4.4705e-01,  2.0108e-01,  7.0588e-01,  4.2728e-01,\n",
            "          4.0357e-01,  2.8297e-02, -1.2254e-01,  1.4269e-01,  2.8105e-04,\n",
            "         -9.9881e-01,  2.0112e-01,  2.6442e-01, -4.1158e-01,  4.2254e-01,\n",
            "         -4.9471e-01,  1.7114e-01, -9.3083e-01,  1.2310e-02, -1.3294e-01,\n",
            "         -4.5130e-01, -4.2558e-01, -1.1234e-01,  1.9889e-01,  5.3867e-01,\n",
            "         -2.3381e-01,  7.7804e-01,  4.7250e-01,  6.3082e-01,  2.6745e-01,\n",
            "          4.3207e-01, -5.2497e-01,  8.2078e-01]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- outputs.pooler_output 을 문장 표현(embedding)으로 사용 가능"
      ],
      "metadata": {
        "id": "nNQDfYh5800u"
      }
    }
  ]
}