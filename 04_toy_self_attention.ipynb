{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPhOapemYzoWwGRp9gXnEtN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monya-9/deep-learning-practice/blob/main/04_toy_self_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Toy Self-Attention 구현"
      ],
      "metadata": {
        "id": "rssOEy4ZTEYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "GwCrjTYHTG_G"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Scaled Dot-Product Attention (단일 헤드)\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Q, K, V를 받아 attention(Q,K,V) = softmax(QK^T / sqrt(d_k)) V 를 계산\n",
        "    - attn_weights: (B, T_q, T_k)\n",
        "    - output:       (B, T_q, d_v)  (여기서는 d_v = d_k = head_dim 가정)\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        # Q,K,V: (B, T, D)\n",
        "        d_k = Q.size(-1)\n",
        "        # (B, T, D) @ (B, D, T) -> (B, T, T)\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (d_k ** 0.5)\n",
        "\n",
        "        if mask is not None:\n",
        "            # mask==0 위치를 매우 작은 값으로 채워 softmax에서 제외\n",
        "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
        "\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "        # (B, T, T) @ (B, T, D) -> (B, T, D)\n",
        "        output = torch.matmul(attn_weights, V)\n",
        "        return output, attn_weights"
      ],
      "metadata": {
        "id": "ZpXsnOHsTU-a"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- scores: Q와 K의 내적 결과 → 유사도 측정.\n",
        "\n",
        "- softmax: 토큰 간 가중치 분포로 변환.\n",
        "\n",
        "- masked_fill: 마스크가 있으면 특정 위치를 무시(-inf).\n",
        "\n",
        "- @ V: 가중합 결과 반환."
      ],
      "metadata": {
        "id": "o563LPmoU7DR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Multi-Head Self-Attention (간단 구현)\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    - d_model: 토큰 임베딩 차원 (예: 128)\n",
        "    - num_heads: 헤드 수 (예: 4, 8 등)\n",
        "    - head_dim = d_model // num_heads 이어야 함\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model=128, num_heads=4, dropout=0.0):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model은 num_heads로 나누어 떨어져야 합니다.\"\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_model // num_heads\n",
        "\n",
        "        # Q/K/V 투영\n",
        "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "        # 헤드 결합 후 최종 출력 투영\n",
        "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "        self.attn = ScaledDotProductAttention(dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
        "\n",
        "    def _split_heads(self, x):\n",
        "        # x: (B, T, d_model) -> (B, num_heads, T, head_dim)\n",
        "        B, T, _ = x.size()\n",
        "        x = x.view(B, T, self.num_heads, self.head_dim)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def _merge_heads(self, x):\n",
        "        # x: (B, num_heads, T, head_dim) -> (B, T, d_model)\n",
        "        B, H, T, D = x.size()\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        return x.view(B, T, H * D)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        x: (B, T, d_model)\n",
        "        mask: (B, 1, 1, T) 또는 (B, 1, T, T) 형태를 지원 (여기서는 생략 가능)\n",
        "        \"\"\"\n",
        "        # 1) 선형 투영\n",
        "        Q = self.W_q(x)  # (B, T, d_model)\n",
        "        K = self.W_k(x)\n",
        "        V = self.W_v(x)\n",
        "\n",
        "        # 2) 헤드 분할\n",
        "        Q = self._split_heads(Q)  # (B, H, T, D)\n",
        "        K = self._split_heads(K)  # (B, H, T, D)\n",
        "        V = self._split_heads(V)  # (B, H, T, D)\n",
        "\n",
        "        # 3) 어텐션 계산 (헤드별 처리)\n",
        "        # ScaledDotProductAttention은 (B, T, D) 입력을 기대하므로,\n",
        "        # 헤드 차원을 배치로 합쳐서 계산하거나, 각 헤드를 loop로 처리할 수 있음.\n",
        "        B, H, T, D = Q.size()\n",
        "        Qf = Q.reshape(B * H, T, D)\n",
        "        Kf = K.reshape(B * H, T, D)\n",
        "        Vf = V.reshape(B * H, T, D)\n",
        "\n",
        "        if mask is not None:\n",
        "            # mask를 (B, 1, 1, T) -> (B*H, 1, T) 형태로 맞춤\n",
        "            mask = mask.expand(B, H, mask.size(-2), mask.size(-1)).reshape(B * H, mask.size(-2), mask.size(-1))\n",
        "\n",
        "        out, attn = self.attn(Qf, Kf, Vf, mask=mask)  # out: (B*H, T, D)\n",
        "        out = out.view(B, H, T, D)\n",
        "\n",
        "        # 4) 헤드 결합 + 최종 투영\n",
        "        out = self._merge_heads(out)              # (B, T, d_model)\n",
        "        out = self.W_o(out)                       # (B, T, d_model)\n",
        "        out = self.dropout(out)\n",
        "        return out, attn.view(B, H, T, T)         # attn: (B, H, T, T)"
      ],
      "metadata": {
        "id": "hjS6I6hsTcXu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Q/K/V 선형 변환\n",
        "\n",
        "- self.W_q, self.W_k, self.W_v: 임베딩 공간에서 Q/K/V를 만드는 projection layer.\n",
        "\n",
        "2. 헤드 분리 (_split_heads)\n",
        "\n",
        "- (B, T, d_model) → (B, num_heads, T, head_dim)\n",
        "\n",
        "3. 스케일드 닷프로덕트 어텐션 계산\n",
        "\n",
        "- 각 head별로 Q/K/V를 넣어 ScaledDotProductAttention 실행.\n",
        "\n",
        "4. 헤드 결합 (_merge_heads)\n",
        "\n",
        "- (B, num_heads, T, head_dim) → (B, T, d_model)\n",
        "\n",
        "5. 출력 투영\n",
        "\n",
        "- self.W_o: 여러 헤드 결합 결과를 다시 하나의 임베딩 공간으로 투영."
      ],
      "metadata": {
        "id": "JBnOhWAfVTDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 간단 사용\n",
        "B, T, D = 2, 5, 128   # 배치=2, 토큰길이=5, 임베딩=128\n",
        "x = torch.randn(B, T, D).to(device)\n",
        "\n",
        "mha = MultiHeadSelfAttention(d_model=D, num_heads=4, dropout=0.1).to(device)\n",
        "y, attn = mha(x)  # y: (B, T, D), attn: (B, H, T, T)\n",
        "\n",
        "print(\"입력 x:\", x.shape)\n",
        "print(\"출력 y:\", y.shape)\n",
        "print(\"어텐션 가중치 attn:\", attn.shape)  # (배치, 헤드, 토큰, 토큰)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnil506TTj68",
        "outputId": "03111452-e676-4108-e38e-ee99c9d37fc9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 x: torch.Size([2, 5, 128])\n",
            "출력 y: torch.Size([2, 5, 128])\n",
            "어텐션 가중치 attn: torch.Size([2, 4, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "입력\n",
        "\n",
        "-  x: (B, T, d_model)\n",
        "B: 배치 크기, T: 시퀀스 길이, d_model: 임베딩 차원.\n",
        "\n",
        "출력\n",
        "\n",
        "- y: (B, T, d_model)\n",
        "self-attention 적용 후 동일한 차원 유지.\n",
        "\n",
        "- attn: (B, num_heads, T, T)\n",
        "각 헤드별 토큰 간 attention score."
      ],
      "metadata": {
        "id": "MwojZmuDV4Po"
      }
    }
  ]
}