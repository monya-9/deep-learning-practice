{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWBIBB5j+3Y2s6qnJkfkgQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monya-9/deep-learning-practice/blob/main/10_tokenizer_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HuggingFace Tokenizer로 문장 토크나이징 실습"
      ],
      "metadata": {
        "id": "SB8z2ppPql7M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYTU4WpJqdz4",
        "outputId": "55414171-82fc-415f-ceda-5a8686c75814"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "# 1. 라이브러리 설치\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 토크나이저 불러오기\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255,
          "referenced_widgets": [
            "7bb275f4a5b744b99bfdcc63c1ba83cd",
            "5d7ce451707c4bec85da64444295a312",
            "1e02decead6345c497409ee2472a7fe7",
            "92232616f6c34152a97a5eb3ff194da6",
            "204850fe8b074010ab44c8f6556b3867",
            "e1ff7b0414304256ac92a3796bf93c0a",
            "4f8103d229664a739b5a2bde5d52417f",
            "c6f5c7b8e4814920ace516b76933b782",
            "08f0310305624aa8afb6678b80ce7246",
            "f5f342806e4b4b25b7b4f22206eee762",
            "e8892423177d4a6482260414d823de75",
            "6ab4bd8c3a094226ad98e7ddaa493a09",
            "8e1928a27f394ea2a7062d128ca2b7cf",
            "e4b9ec8063f84e69a19504f85f14ede3",
            "81cc9e90da5940109871feb4ff748dac",
            "0cce3d6780994e30b38ba93052a71396",
            "aa5035a7f6df4963a37701515ca7a361",
            "aff915a9d92a472286c54f5ea57ea428",
            "97b6e58b100340399c38c8b7b09de6dc",
            "cc23c171091e45edaef1c580d8811f9a",
            "ab7f9f262afb446298f818f0619c8cde",
            "05b39a16f9544d218dd590aefd2bdaab",
            "f00651d391e24ebc856f4b32d33124f2",
            "3763629971584887bce35ffdd66339e5",
            "080f5b46ee6e4e72b65b17cfbd78e6c5",
            "d962c14eeed24bd8bb738ae9e4ae9ed9",
            "a82808baebe14386b791fcd47417deef",
            "a6cc813e4e594ee0b5569c9bbc63f499",
            "6b6bb7a04cbc40f38b2263ca6be78d12",
            "3f6c31dedfb34208844196a8bff846c9",
            "a7c3f52da6b145ea9b462d87bdae9bec",
            "4bbcf1727df7455e8aa5d4d2e6cc1530",
            "c9f64c54fc0e4c3f98a96c672cb3621b",
            "afcc3b178b1c46eab34feb46f0b8f094",
            "7acd8bf22f7f485ba76638440039d5b5",
            "884e6ec0384d4e2e913cbd79bd764c59",
            "619ce38037ff403594664668fd921181",
            "54b5df512b4e4c54aef1304c89779701",
            "97c4753adc44403fbdf863be4bd0a017",
            "ec96c2bb8b594557859c1084a39a26b9",
            "2747fe6e6d67484eb2747dffbca40742",
            "f91508413e154aa18e6e55bc64530965",
            "a475c3a2e93d472e8fc297d79083b47c",
            "66b352a8bbb24abfa3c24aa900bc4359"
          ]
        },
        "id": "2U4u_7jyqwgg",
        "outputId": "55c22743-169e-4b7f-a481-8ce674270502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7bb275f4a5b744b99bfdcc63c1ba83cd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ab4bd8c3a094226ad98e7ddaa493a09"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f00651d391e24ebc856f4b32d33124f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "afcc3b178b1c46eab34feb46f0b8f094"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Hugging Face Hub에서 ERRT-base 모델의 토크나이저 다운로드"
      ],
      "metadata": {
        "id": "rGH0IbKvrH1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 단일 문장 토크나이징\n",
        "sentence = \"Hello Hugging Face Tokenizer practice!\"\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(\"Tokenized:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWJxgZifqxXf",
        "outputId": "e7c1fc4d-791a-4590-b6e8-b4c47f565f6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized: ['hello', 'hugging', 'face', 'token', '##izer', 'practice', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 단어를 WordPiece 단위로 분리 (Tokenizer -> token ##izer)"
      ],
      "metadata": {
        "id": "k9f_Kzp1rL7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 토큰 → 숫자 ID 변환\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(\"Token IDs:\", token_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9V0wU_FOqyN7",
        "outputId": "cfd9d4c9-e7f9-42e5-bdd8-e343f6987fb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs: [7592, 17662, 2227, 19204, 17629, 3218, 999]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 모델이 이해할 수 있는 정수 시퀀스로 변환"
      ],
      "metadata": {
        "id": "HVFwLbx8rUwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 문장을 바로 인코딩 (special token 포함)\n",
        "encoding = tokenizer.encode(sentence)\n",
        "print(\"Encoding (with special tokens):\", encoding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiRqKpBGqy67",
        "outputId": "d8ef80f4-ec49-49f1-ee8c-3dfc3fb4c6b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding (with special tokens): [101, 7592, 17662, 2227, 19204, 17629, 3218, 999, 102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [CLS]=101, [SEP]=102 토큰 자동 추가"
      ],
      "metadata": {
        "id": "2okN0OXKrTHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. 다시 디코딩\n",
        "decoded = tokenizer.decode(encoding)\n",
        "print(\"Decoded:\", decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM2reBwYqzvY",
        "outputId": "c771098a-136a-4c34-b642-5428ca2189c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoded: [CLS] hello hugging face tokenizer practice! [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 다시 사람이 읽을 수 있는 문자열로 변환"
      ],
      "metadata": {
        "id": "u0gHvrsSrclA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 7. 여러 문장 한 번에 토크나이징 (배치 인코딩)\n",
        "sentences = [\n",
        "    \"Hugging Face is great!\",\n",
        "    \"Tokenizers make NLP easy.\"\n",
        "]\n",
        "batch_encoding = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(\"Batch Encoding:\", batch_encoding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixq-3sUtq0pH",
        "outputId": "e29c613a-2f83-424f-ebd5-4f505902813d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch Encoding: {'input_ids': tensor([[  101, 17662,  2227,  2003,  2307,   999,   102,     0,     0,     0],\n",
            "        [  101, 19204, 17629,  2015,  2191, 17953,  2361,  3733,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 패딩과 잘라내기 옵션 적용\n",
        "- PyTorch Tensor 형식으로 반환(input_ids, attention_mask 등 포함)"
      ],
      "metadata": {
        "id": "yyJPjh7drezC"
      }
    }
  ]
}