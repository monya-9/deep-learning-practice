{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqpQSM5/Dd1hRo0B3LVIvT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monya-9/deep-learning-practice/blob/main/09_BERT_FineTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 사전학습된 BERT에 fine-tuning 해보기 (감정 분석 등)"
      ],
      "metadata": {
        "id": "7MS25UQ3ZgzA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25JNegyVZQsN",
        "outputId": "3305eee8-5412-43de-e4da-6f9d7b1c8d45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.55.2\n",
            "Uninstalling transformers-4.55.2:\n",
            "  Successfully uninstalled transformers-4.55.2\n",
            "Found existing installation: datasets 4.0.0\n",
            "Uninstalling datasets-4.0.0:\n",
            "  Successfully uninstalled datasets-4.0.0\n",
            "\u001b[33mWARNING: Skipping evaluate as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting transformers\n",
            "  Downloading transformers-4.55.4-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading transformers-4.55.4-py3-none-any.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m251.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m346.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m330.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers, datasets, evaluate\n",
            "Successfully installed datasets-4.0.0 evaluate-0.4.5 transformers-4.55.4\n"
          ]
        }
      ],
      "source": [
        "# 1. 라이브러리 설치\n",
        "# 기존 패키지를 강제로 삭제합니다.\n",
        "!pip uninstall -y transformers datasets evaluate\n",
        "\n",
        "# 캐시를 무시하고 최신 버전을 다시 설치합니다.\n",
        "!pip install --no-cache-dir --upgrade transformers datasets evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 라이브러리 불러오기\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "_vLN4O0lZyFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 데이터셋 로드 (IMDb 영화 리뷰 감정 분석) 및 토크나이\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337,
          "referenced_widgets": [
            "04c7af485b1f49479f7ccf87d2314039",
            "b91ab4652ea44ffa9442cd624dce7cc1",
            "bb6813433f8d49a7a99e4fe066d7d005",
            "10fcbcb5f97f4556b84946321cd5d99e",
            "84d204acd7934c6f94e4da28bd560b18",
            "12b8630971c24fe6a87cf29d2f18612f",
            "4e4b217e14e049e8bde56a02d545b0de",
            "c8d88143a8eb47ec9f9512e92ce324c3",
            "439ca43bb2fc48cca6c0c45740bc2f4e",
            "473e76834e84466d9b6ea408163e706c",
            "6d6eec3a7fdf46e0982445fb216f9481",
            "b9503a0823dd45ad843d98c5a030514e",
            "817f92c6c82d497c960e630ac6a102a4",
            "d25bf3fea91048bc9cf566d8d555c667",
            "9ad012a4f4a64647a0f177a650a2b515",
            "89298139a2524f3bb727bf9574156b44",
            "96b66f1e595145bdad7ec3692747efc6",
            "0a244bbbbe07443a9ece60d0308bec41",
            "588149fd785844ef9b1dc677ee3d1d67",
            "2dce5c35144d4670a9cf3d0f8c502e3f",
            "1f37a5353dc14ca799cbdfa72d4a56c0",
            "56de9415b04f4326a6839ad427835d98",
            "4a1bdab1ebe549cc87387b6f256da0e8",
            "17bc0e1866404602a7923cddb8267999",
            "9137ae358ea24a8d875f391d6575c07d",
            "0dc478146a444225a4a5b421fdd99766",
            "8dccd2301549467aacb821affe0bb876",
            "4346edf9775e41e6bbda496cd8ac98a8",
            "a8feac01315743409616470a81de551d",
            "7998055df9444909ab4d89cf3523ed8c",
            "3e95dd2134a4404599106ddc621b6ff9",
            "66d2533ce97a425493c11b0e8cf39186",
            "273a57063343491c8cace4e212a40469",
            "d0bba15450eb411da457e1c9b654e70a",
            "bea000fecde244de9ff8253652f65f5f",
            "1d275e25717a495fb97982d42c712add",
            "3476fefc2e644cfdb7f9dff19c73b009",
            "3fca51d1ce2c45b8ac23a7c7728e45f2",
            "63da830b3ca141c2b864c8be4313dcae",
            "a56554b9acbd44709f9d363d0b0eb05e",
            "28acb1b837494e07adf837e04943e988",
            "2a99c6e55453433dbe42355ff0563761",
            "cd65cf37e3674400a13079c628df2b5d",
            "64b75ec7cd4c42cf9ea8ce6a078b4026",
            "1584d75b01384d3da4fdc05f80f58c4d",
            "d591d34ce01340eba8afebe7eaac27bf",
            "9faeb681a97a4f6dab529fd3e279b61c",
            "0a10e7a609084d278b5dcb742864d2f4",
            "5ca59f9d0fc440a3b73235ddec656de2",
            "4aaa4435a6694ed2ad14d4793798c398",
            "75605732dd854d518fb8f5467eb0f8d0",
            "ed611e6cd2b5498697d6537c3a56d6a4",
            "0756fbb9351e44a0855daaa87be21f09",
            "80cd7ce01d4d418aae56f2a8bf6faef6",
            "a92e3f761516474d81d44bf69489fc2e",
            "f5eaef063e734054b235ef902aa2bc81",
            "2df440836a784f0a9d116f11f05ef22c",
            "9d549db1238a457b98fa605e9c5626a8",
            "6894802fb6e94f3a8427a39262da24aa",
            "d081627e51814a56a1f97b2364336c73",
            "f9f606aede304888a31ad85137cf5493",
            "d2f9d13baaec401f9ba76c33844a15cb",
            "500aa5b338ff44c89ad461bb1b07c4b6",
            "155c389a9b784a03a50f9470e9671f18",
            "d1d4234e04b54074b4308ba3e17c2c51",
            "bbd5592303f74d229e15f3d88cac99cd",
            "dc498dcbf8284887bb52b6fbfbed7164",
            "7039a9bc983846d78be0af1117434e43",
            "982dc3de37824946a3e0de9ee77d8406",
            "4cfd53d9cdaf455babaa79e03e0c13d3",
            "af6dd26008234c169b12ca4e394e8902",
            "88fe184b66bc44e4ae67c7f75f4c30e3",
            "72d41008c3884bbaab30f3b29ab1ed0c",
            "ea1c48f0f79c40ccab03615197fd464c",
            "1e13232cb3e74b368ed6fa6e09a34a08",
            "c89dc66832e9439ea4f3a1cbd939cd31",
            "7f4c28d0d3c345399d3949a0d4910c3b",
            "767f28aac4fd4c59b1b1fb82f7201af7",
            "716ff68d7ba848afb29bde1bca8c4c11",
            "a2e2e69a3435466cbbc7e33e298f3156",
            "9ff8f313ea0e45dc9e27bfd355bf55c9",
            "3caceecc06d848899c44cf8424d51d2a",
            "1cf865f6cb9e4f4d9f8897130faa80b9",
            "994047de21c54c11888b2fa207aa14f0",
            "a88530895fc14fbca5d7535cbca01296",
            "640ea66b231a483a9c85f59303c9dd1f",
            "183fefc3b4fe41c59fd1e72d54a286c8",
            "bee4e1efbdc848c9a71a9365b7f0a27a",
            "815f3dbd968e4ec3b652f5305712497f",
            "782cab52d29f4d8e95f3a80aedac0a16",
            "81d003c972a446d397552b264a03c3fe",
            "28666be1acd94f25b15fdb3e36589d9e",
            "7575728fec1d45fdaaf910bceaa47365",
            "708e2b755732428c9c301bacdb3d35fa",
            "7e27aff7e3b245c0bc1d136ed4248950",
            "57aaef51eae84841b8c344111cc6fe08",
            "4e307ef1f1d0467a879e736749dfcaf3",
            "e28ad28b89824dbcbcefbb41153d375b",
            "87d4f406ef7941bb97dd9a466d71dfeb",
            "43da716e7db44f0f830dd62f79930b81",
            "9af5d846e0814e4092d6c31ee5af6fdf",
            "9b7e0481545543549d33a325ac0579fa",
            "205268770d634bfbae0098acff862801",
            "8b595e8b9ed5410680f0da72dec9b229",
            "abefa079344a499ea6792ebe492ad19e",
            "ac63e3e7f96e4474b161d7ba7c169867",
            "7c137e846e6d4d328455054587c1b7c9",
            "91b209bf668c4b04a99a7554378ebdd8",
            "40fce2ca204243fc9f62cd1d16ffe622",
            "15df5066c4e34ff88f567f0cec5d8005"
          ]
        },
        "id": "RyD3EBoMZ3ar",
        "outputId": "85120fa8-031e-48d6-8ac4-98695326be78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04c7af485b1f49479f7ccf87d2314039"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b9503a0823dd45ad843d98c5a030514e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a1bdab1ebe549cc87387b6f256da0e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0bba15450eb411da457e1c9b654e70a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1584d75b01384d3da4fdc05f80f58c4d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5eaef063e734054b235ef902aa2bc81"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc498dcbf8284887bb52b6fbfbed7164"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "767f28aac4fd4c59b1b1fb82f7201af7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "815f3dbd968e4ec3b652f5305712497f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43da716e7db44f0f830dd62f79930b81"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- IMDb 영화 리뷰 데이터셋 로드\n",
        "- AutoTokenizer로 BERT 토크나이저 로드 (bert-base-uncased 사용, 대소문자 구분x)\n",
        "- 각 리뷰를 토큰화(tokenization) 하고, 최대 길이 256으로 패딩/자르기 수행.\n",
        "- dataset.map으로 전체 데이터셋에 적용"
      ],
      "metadata": {
        "id": "NJAvKc8_o8yj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 데이터셋 분리\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(5000))  # 빠른 학습용 소규모\n",
        "small_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(2000))"
      ],
      "metadata": {
        "id": "KziwbUSqjNPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 학습 속도를 빠르게 하기 위해 작은 샘플로 학습/테스트셋 생성\n",
        "- shuffle(seed=42)로 데이터 섞기, select(range(N))로 상위 N개 선택"
      ],
      "metadata": {
        "id": "BS0RKqqppMgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. BERT 모델 로드 및 Fine-tuning\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "\n",
        "# 평가 지표 (Accuracy)\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# 학습 설정\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"test_trainer\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=50,\n",
        "    push_to_hub=False,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Trainer 준비\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# 학습 실행\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "9rD0qzxyZ6hU",
        "outputId": "fde6e6fb-eeee-4cf3-8dbc-4de10ef47bf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-4051845849.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='626' max='626' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [626/626 08:19, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.584300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.355400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.309700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.328400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.296600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.315600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.190700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.152300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.217700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.211400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.175400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.175500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=626, training_loss=0.27120188630807895, metrics={'train_runtime': 501.8516, 'train_samples_per_second': 19.926, 'train_steps_per_second': 1.247, 'total_flos': 1315555276800000.0, 'train_loss': 0.27120188630807895, 'epoch': 2.0})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. BERT 모델 로드\n",
        "  - 사전 학습된 BERT 모델 불러오기\n",
        "  - 감정 분석이므로 클래스 수를 2로 설정(Positive / Negative)\n",
        "2. 평가 지표 정의\n",
        "  - 정확도(Accuracy)를 평가 지표로 사용\n",
        "  - 모델 출력 logits에서 argmax로 예측 라벨 추출 후, 실제 레이블과 비교\n",
        "3. 학습 설정\n",
        "  -  학습 관련 하이퍼파라미터 설정\n",
        "    - 학습률 2e-5, 배치 사이즈 16, 2 epoch 학습\n",
        "    - 가중치 감쇠(weight_decay) 적용\n",
        "    - 로그 저장 디렉토리와 로깅 간격 지정\n",
        "    - save_strategy=\"epoch\": epoch마다 모델 저장\n",
        "    - report_to=\"none\": wandb 등 외부 기록 비활성화\n",
        "4. Trainer 객체 생성\n",
        "  - Hugging Face Trainer는 학습 루프, 평가, 체크포인트 저장 등을 자동으로 처리\n",
        "  - 토크나이저와 평가 함수(compute_metrics)도 전달.\n",
        "5. 학습 실행\n",
        "  - BERT를 IMDb 샘플 데이터에 맞춰 Fine-tuning 수행"
      ],
      "metadata": {
        "id": "A7_CMVgQpYLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. 평가 실행 및 추론\n",
        "# 입력 문장을 토크나이징\n",
        "text = \"The movie was fantastic! I really loved it.\" # Positive 😀\n",
        "# text = \"The movie was terrible and a complete waste of time.\" #Negative 😡\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "# 모델과 같은 디바이스로 이동\n",
        "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "# 추론\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "\n",
        "pred = torch.argmax(logits, dim=-1).item()\n",
        "print(\"입력 문장:\", text)\n",
        "print(\"예측 결과:\", \"Positive 😀\" if pred == 1 else \"Negative 😡\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9PKp_syjapZ",
        "outputId": "48beaed1-327d-4e72-e2ac-13b9bfeb881e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "입력 문장: The movie was fantastic! I really loved it.\n",
            "예측 결과: Positive 😀\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 임의 문장 하나를 토크나이징 후, 모델 디바이스(CPU/GPU)로 이동\n",
        "- torch.no_grad()로 추론 모드에서 연산\n",
        "- logits에서 argmax로 예착 라벨 선택\n",
        "- 결과를 Positive 😀 / Negative 😡로 출력"
      ],
      "metadata": {
        "id": "ryl-hPFxqRBQ"
      }
    }
  ]
}