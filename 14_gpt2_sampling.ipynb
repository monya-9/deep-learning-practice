{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMfHbIESl46ksbuzuybo4pq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monya-9/deep-learning-practice/blob/main/14_gpt2_sampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-2로 문장 생성 실험, Sampling 방식 변경"
      ],
      "metadata": {
        "id": "Wgh6R-DU-OP5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U0ud8Qk7-HMI"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 모델 & 토크나이저 불러오기\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyrr2gBm-UP1",
        "outputId": "62622b1f-bbd9-49e9-aecf-e5e41478c87c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 입력 문장\n",
        "prompt = \"Artificial intelligence will change the future of\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids"
      ],
      "metadata": {
        "id": "JuF0sw7v-WBQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 다양한 Sampling 방식 실험\n",
        "# (1) Greedy Search\n",
        "greedy_output = model.generate(\n",
        "    inputs,\n",
        "    max_length=50\n",
        ")\n",
        "\n",
        "# (2) Sampling (temperature 낮음 → 덜 창의적)\n",
        "sample_output_temp_low = model.generate(\n",
        "    inputs,\n",
        "    max_length=50,\n",
        "    do_sample=True,\n",
        "    temperature=0.7,\n",
        "    num_return_sequences=2\n",
        ")\n",
        "\n",
        "# (3) Sampling (temperature 높음 → 더 창의적)\n",
        "sample_output_temp_high = model.generate(\n",
        "    inputs,\n",
        "    max_length=50,\n",
        "    do_sample=True,\n",
        "    temperature=1.5,\n",
        "    num_return_sequences=2\n",
        ")\n",
        "\n",
        "# (4) Top-k Sampling (상위 k개 단어에서만 선택)\n",
        "sample_output_topk = model.generate(\n",
        "    inputs,\n",
        "    max_length=50,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    num_return_sequences=2\n",
        ")\n",
        "\n",
        "# (5) Nucleus Sampling (top-p, 확률 누적 0.9까지 고려)\n",
        "sample_output_topp = model.generate(\n",
        "    inputs,\n",
        "    max_length=50,\n",
        "    do_sample=True,\n",
        "    top_p=0.9,\n",
        "    num_return_sequences=2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uZYlVa0-XLD",
        "outputId": "f92dc717-4dc5-47ec-fea0-e2e9c43775a3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Greedy Search: 항상 가장 확률 높은 단어만 선택 → 무난하지만 반복/단조로움.\n",
        "\n",
        "- Temperature:\n",
        "\n",
        "  - 0.7 → 보수적, 안정적인 결과\n",
        "\n",
        "  - 1.5 → 창의적, 예측 불가한 결과\n",
        "\n",
        "- Top-k Sampling: 확률 상위 k개만 후보로 → 불필요한 단어 제거, 다양성 확보.\n",
        "\n",
        "- Nucleus Sampling (Top-p): 확률 누적이 p(예: 0.9) 미만인 단어만 후보 → 상황에 맞는 균형 잡힌 결과."
      ],
      "metadata": {
        "id": "V1ifhMlY_OAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 결과 출력\n",
        "print(\"=== Greedy ===\")\n",
        "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))\n",
        "\n",
        "print(\"\\n=== Sampling (temperature=0.7) ===\")\n",
        "for out in sample_output_temp_low:\n",
        "    print(tokenizer.decode(out, skip_special_tokens=True))\n",
        "\n",
        "print(\"\\n=== Sampling (temperature=1.5) ===\")\n",
        "for out in sample_output_temp_high:\n",
        "    print(tokenizer.decode(out, skip_special_tokens=True))\n",
        "\n",
        "print(\"\\n=== Top-k Sampling (k=50) ===\")\n",
        "for out in sample_output_topk:\n",
        "    print(tokenizer.decode(out, skip_special_tokens=True))\n",
        "\n",
        "print(\"\\n=== Nucleus Sampling (top-p=0.9) ===\")\n",
        "for out in sample_output_topp:\n",
        "    print(tokenizer.decode(out, skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1nPNIm8-eGT",
        "outputId": "b00a58b2-360b-4989-a980-0fe5d3c4eba2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Greedy ===\n",
            "Artificial intelligence will change the future of the world.\n",
            "\n",
            "The future of the world is not yet clear. But it is clear that AI will change the world.\n",
            "\n",
            "The future of the world is not yet clear. But it is clear\n",
            "\n",
            "=== Sampling (temperature=0.7) ===\n",
            "Artificial intelligence will change the future of the world, and we can't wait to see where it goes next.\n",
            "\n",
            "The article was originally published on The Conversation. Read the original article.\n",
            "Artificial intelligence will change the future of life, not just the present.\n",
            "\n",
            "=== Sampling (temperature=1.5) ===\n",
            "Artificial intelligence will change the future of healthcare in ways that many experts view as devastating by comparison.\"\n",
            "\n",
            "\n",
            "Read or Share this story: http://usat.ly/1K1WcPV\n",
            "Artificial intelligence will change the future of computing, just long enough to fill all computing jobs (most in computers). It is not just computer programming and algorithms we need to pay attention to—it makes the game that much much better.\"\n",
            "\n",
            "At\n",
            "\n",
            "=== Top-k Sampling (k=50) ===\n",
            "Artificial intelligence will change the future of the field, but not for the better. It may even have other effects on the future, including how it interacts with things like the Internet, and on how we might actually use technology. While the best AI\n",
            "Artificial intelligence will change the future of everything\n",
            "\n",
            "As technology progresses, it will improve the way we understand the world as we know it, and as we adapt to it to our own needs. Artificial intelligence will help us solve problems better, because\n",
            "\n",
            "=== Nucleus Sampling (top-p=0.9) ===\n",
            "Artificial intelligence will change the future of science and technology, according to a new report published Monday by the National Academies of Sciences and Engineering.\n",
            "\n",
            "In a report published in the journal Nature, the scientists say that a human-computer interface will\n",
            "Artificial intelligence will change the future of our lives because we will become smarter.\n",
            "\n",
            "What is Artificial Intelligence and what does it represent? It is a machine learning paradigm that focuses on creating a better world for its user. It uses real-world\n"
          ]
        }
      ]
    }
  ]
}