{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQAb6l7zlmkOolNjlR+JQC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monya-9/deep-learning-practice/blob/main/17_bert_lora_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LoRA 적용하여 BERT fine-tuning"
      ],
      "metadata": {
        "id": "4NYsGA51_uXZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZkoJmIt_n3p"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets peft accelerate bitsandbytes evaluate -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "Ev2r0Y7V_yEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 1. 데이터 준비\n",
        "# --------------------------\n",
        "dataset = load_dataset(\"imdb\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize, batched=True)\n",
        "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "# 데이터 샘플 줄이기 (빠른 테스트용)\n",
        "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(2000))\n",
        "eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159,
          "referenced_widgets": [
            "f00991c0df4e4133b7083a6b024e617e",
            "79a312d0ee60489195eb97673c9c81f7",
            "b59405cd184f41499a2c4b8e5c0b2d26",
            "9a6362f0435d4ea38209f57e70607b83",
            "ecc0e56420934a39b6d976d6dd67c88f",
            "50abb9715fcd4821a69e799aa5813858",
            "6d0eb01921744dd8870644295a8cbe5d",
            "abaf3a8cd1e84820b655b22e703b1d1e",
            "cb07256ce67142e9bf4bc8441180b6ba",
            "d426adaa8f1d4480a8b9ea225bd9593e",
            "2a93fc169e1b47bd847a32440a65d893"
          ]
        },
        "id": "RYT7sUMM_zBg",
        "outputId": "86cfc8c9-1ece-4bf3-baea-7cb85b300b75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f00991c0df4e4133b7083a6b024e617e"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 2. BERT 모델 불러오기\n",
        "# --------------------------\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmTjB2C6_29B",
        "outputId": "236f4966-120e-45bb-9b54-eedd15b9c052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 3. LoRA 설정\n",
        "# --------------------------\n",
        "lora_config = LoraConfig(\n",
        "    r=8,              # low-rank 차원\n",
        "    lora_alpha=32,    # 스케일링\n",
        "    target_modules=[\"query\", \"value\"],  # 어댑터 적용 대상 모듈\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_CLS\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "xd6mYhsz_4HE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- r=8 : 저차원 적응 행렬 차원\n",
        "\n",
        "- target_modules=[\"query\", \"value\"] : Transformer의 Query/Value 가중치에만 적용\n",
        "\n",
        "- lora_alpha : 학습률 스케일링\n",
        "\n",
        "- 전체 BERT 가중치는 고정 → 메모리 효율 + 빠른 학습"
      ],
      "metadata": {
        "id": "OO2VX4KiBxVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 4. 지표 정의\n",
        "# --------------------------\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "f1 = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = logits.argmax(-1)\n",
        "    return {\n",
        "        \"accuracy\": accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
        "        \"f1\": f1.compute(predictions=preds, references=labels, average=\"weighted\")[\"f1\"]\n",
        "    }"
      ],
      "metadata": {
        "id": "wYtRg8-R_6PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 5. Trainer 정의\n",
        "# --------------------------\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_lora\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=5e-4,\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"none\",  # W&B 비활성화\n",
        "    fp16=True           # GPU mixed precision\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLPN_L3o_7oK",
        "outputId": "976e14f3-f521-4e60-a651-b692066556f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-83397586.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- HuggingFace Trainer 그대로 사용 가능\n",
        "\n",
        "- LoRA 모델도 동일 인터페이스로 학습 가능"
      ],
      "metadata": {
        "id": "G5o4nFU2B2DC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------\n",
        "# 6. 학습 + 평가\n",
        "# --------------------------\n",
        "trainer.train()\n",
        "results = trainer.evaluate()\n",
        "print(\"\\n✅ 평가 결과:\", results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "HZHrZQaZ_8re",
        "outputId": "0cc03cec-0942-4f5a-8e7b-8e192506cfe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 00:32, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.390441</td>\n",
              "      <td>0.820000</td>\n",
              "      <td>0.820023</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:04]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ 평가 결과: {'eval_loss': 0.3904406428337097, 'eval_accuracy': 0.82, 'eval_f1': 0.8200230414746544, 'eval_runtime': 4.5483, 'eval_samples_per_second': 219.864, 'eval_steps_per_second': 27.483, 'epoch': 1.0}\n"
          ]
        }
      ]
    }
  ]
}